{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"2. Create Training Data from Features \"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßëüèª‚Äçüè´ HSFS `Feature Views` and `Training Datasets`\n",
    "\n",
    "`Feature Views` is the third building block of the Hopsworks Feature Store. Feature Views store metadata of our dataset.\n",
    "\n",
    "`Training datasets` is the fourth building block of the Hopsworks Feature Store. \n",
    "\n",
    "Training datasets can be saved in a ML framework friendly format (eg. TfRecords, CSV, Numpy) and then be fed to a machine learning model for training.\n",
    "\n",
    "Training datasets can also be stored on external storage systems like Amazon S3 or GCS to be read by external model training platforms.\n",
    "\n",
    "As with the previous notebooks, the first step is to establish a connection with the Hopsworks feature store and get the feature store handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>30</td><td>application_1653473648291_0126</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-17-208.eu-north-1.compute.internal:8089/proxy/application_1653473648291_0126/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-16-76.eu-north-1.compute.internal:8044/node/containerlogs/container_e01_1653473648291_0126_01_000001/Basics__maksym00\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Create a `Feature View` from a query\n",
    "\n",
    "In the previous notebook ([feature_exploration](./feature_exploration.ipynb)) we walked through how to explore and query the Hopsworks feature store using HSFS. We can use the queries produced in the previous notebook to create a `Feature Views`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_fg = fs.get_feature_group(\n",
    "    name = 'sales_fg',\n",
    "    version = 1\n",
    ")\n",
    "\n",
    "exogenous_fg = fs.get_feature_group(\n",
    "    name = 'exogenous_fg',\n",
    "    version = 2\n",
    ")\n",
    "\n",
    "query = sales_fg.select_all()\\\n",
    "        .join(exogenous_fg.select(['fuel_price', 'unemployment', 'cpi']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create Feature View we can use `FeatureStore.create_feature_view()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name = 'exodenous_sale',\n",
    "    version = 1,\n",
    "    query = query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_view.FeatureView object at 0x7fa0d5e6dbe0>"
     ]
    }
   ],
   "source": [
    "feature_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now `Feature View` is saved in Hopsworks and we can retrieve it using `FeatureStore.get_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\n",
    "    name = 'exodenous_sale',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "feature_view.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `FeatureView.preview_feature_vector()` returns a sample of assembled serving vector from online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "feature_view.preview_feature_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To get subset of data use `FeatureView.get_batch_data()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch = feature_view.get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(df_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "|fuel_price|unemployment|        cpi|\n",
      "+----------+------------+-----------+\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "+----------+------------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_batch.select(['fuel_price', 'unemployment', 'cpi']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßëüèª‚Äçüî¨ Training Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create training dataset we use `FeatureView.create_training_dataset()` method.\n",
    "\n",
    "‚ö†Ô∏è Some important things:\n",
    "- It will inherit the name of FeatureView.\n",
    "\n",
    "- The feature store currently supports the following data formats for\n",
    "training datasets: **tfrecord, csv, tsv, parquet, avro, orc**.\n",
    "\n",
    "- We can choose necessary format using **data_format** parameter.\n",
    "\n",
    "- Also we can specify split ratio using **splits** parameter.\n",
    "\n",
    "- **train_split** - specify which split will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_view.create_training_dataset(\n",
    "    version = 1,\n",
    "    description = 'trial_dataset',\n",
    "    data_format = 'csv',\n",
    "    splits = {'train': 80, 'validation': 20},\n",
    "    train_split = \"train\",\n",
    "    write_options = {'wait_for_job': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None)"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to load dataset from Hopsworks we can use `FeatureView.get_training_dataset_splits()` method.\n",
    "\n",
    "By specifying **splits** parameter we can choose what split of training dataset to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------+----------+--------------------------+\n",
      "|store|dept|      date|weekly_sales|is_holiday|sales_last_month_store_dep|\n",
      "+-----+----+----------+------------+----------+--------------------------+\n",
      "|    1|   1|2010-03-05|     21827.9|     false|                 131963.08|\n",
      "|    1|  10|2010-03-05|    33299.27|     false|        119772.36000000002|\n",
      "|    1|  11|2010-03-05|     19082.9|     false|                  81986.75|\n",
      "|    1|  12|2010-03-05|    10239.06|     false|                  35284.96|\n",
      "|    1|  13|2010-03-05|    40423.95|     false|                 153770.69|\n",
      "+-----+----+----------+------------+----------+--------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "td_version, df = feature_view.get_training_dataset_splits(\n",
    "    splits = {},\n",
    "    start_time = None,\n",
    "    end_time = None,\n",
    "    version = 2\n",
    ")\n",
    "\n",
    "df.select(['store', 'dept', 'date', 'weekly_sales', 'is_holiday', 'sales_last_month_store_dep']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ Creating Training Datasets with Event Time filter\n",
    "\n",
    "First of all lets import **datetime** from datetime library and set up a time format.\n",
    "\n",
    "Then we can define start_time point and end_time point.\n",
    "\n",
    "Finally we can create training dataset with data in specific time bourders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(x, '%Y-%m-%d')\n",
    "    dt_obj = dt_obj.timestamp() * 1000\n",
    "    return int(dt_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timestamp_2_time('2008-01-01')\n",
    "end_time = timestamp_2_time('2012-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogenous_fg = fs.get_feature_group(\n",
    "    name = 'exogenous_fg',\n",
    "    version = 1\n",
    ")\n",
    "\n",
    "query = exogenous_fg.select_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogenous_fv = fs.create_feature_view(\n",
    "    name = 'exogenous_fg_2008_2012',\n",
    "    version = 1,\n",
    "    query = query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_version, td_job = exogenous_fv.create_training_dataset(\n",
    "    description = 'exogenous_fg_filtered',\n",
    "    version = 1,\n",
    "    data_format = 'csv',\n",
    "    write_options = {'wait_for_job': True},\n",
    "    start_time = start_time,\n",
    "    end_time = end_time,\n",
    "    statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Dataset Retrieving\n",
    "\n",
    "To retrieve training dataset from Feature Store we can use `get_training_dataset_splits()` or `get_training_dataset()` methods. \n",
    "\n",
    "If version is not provided - new one will be created.\n",
    "If version is provided and version exists - retrieves trainining dataset and returns as dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(store=26, date=1265328000000, temperature=9.55, fuel_price=2.788, markdown1='NA', markdown2='NA', markdown3='NA', markdown4='NA', markdown5='NA', cpi='131.5279032', unemployment='8.488', is_holiday=False)\n",
      "VersionWarning: No version provided for creating training dataset, incremented version to `2`."
     ]
    }
   ],
   "source": [
    "td_version, df = exogenous_fv.get_training_dataset(\n",
    "    start_time = start_time,\n",
    "    end_time = end_time\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}